{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import time\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import logging\n",
    "from copy import deepcopy\n",
    "from collections import namedtuple\n",
    "import torch\n",
    "import pyro\n",
    "\n",
    "from spatial_scene_grammars.nodes import *\n",
    "from spatial_scene_grammars.rules import *\n",
    "from spatial_scene_grammars.scene_grammar import *\n",
    "from spatial_scene_grammars.parsing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observed 8 objects\n",
      "Objects:  [<spatial_scene_grammars_examples.singles_pairs.grammar_constituency.Root object at 0x7f29be3456a0>, <spatial_scene_grammars_examples.singles_pairs.grammar_constituency.Object object at 0x7f29be852b00>, <spatial_scene_grammars_examples.singles_pairs.grammar_constituency.Object object at 0x7f29be7eaf28>, <spatial_scene_grammars_examples.singles_pairs.grammar_constituency.Object object at 0x7f29be7ea6d8>, <spatial_scene_grammars_examples.singles_pairs.grammar_constituency.Object object at 0x7f29be7ead30>, <spatial_scene_grammars_examples.singles_pairs.grammar_constituency.Object object at 0x7f29be7ea7f0>, <spatial_scene_grammars_examples.singles_pairs.grammar_constituency.Object object at 0x7f29be7ea828>, <spatial_scene_grammars_examples.singles_pairs.grammar_constituency.Object object at 0x7f29be7eadd8>]\n"
     ]
    }
   ],
   "source": [
    "from spatial_scene_grammars_examples.singles_pairs.grammar_constituency import *\n",
    "#from spatial_scene_grammars_examples.singles_pairs.grammar_dependency import *\n",
    "pyro.set_rng_seed(42)\n",
    "\n",
    "grammar = SpatialSceneGrammar(\n",
    "    root_node_type = Root,\n",
    "    root_node_tf = torch.eye(4)\n",
    ")\n",
    "ground_truth_tree = grammar.sample_tree(detach=True)\n",
    "observed_nodes = ground_truth_tree.get_observed_nodes()\n",
    "print(\"Observed %d objects\" % len(observed_nodes))\n",
    "print(\"Objects: \", observed_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate <spatial_scene_grammars_examples.singles_pairs.grammar_constituency.Singles object at 0x7f29be864eb8> with pose tensor([[1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.]])\n",
      "Candidate <spatial_scene_grammars_examples.singles_pairs.grammar_constituency.Singles object at 0x7f29be8080b8> with pose tensor([[1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.]])\n",
      "Candidate <spatial_scene_grammars_examples.singles_pairs.grammar_constituency.Singles object at 0x7f29be8087b8> with pose tensor([[1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.]])\n",
      "Candidate <spatial_scene_grammars_examples.singles_pairs.grammar_constituency.Singles object at 0x7f29be808898> with pose tensor([[1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.]])\n",
      "Candidate <spatial_scene_grammars_examples.singles_pairs.grammar_constituency.Singles object at 0x7f29bdb36748> with pose tensor([[1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.]])\n",
      "Candidate <spatial_scene_grammars_examples.singles_pairs.grammar_constituency.Singles object at 0x7f29be7f0748> with pose tensor([[1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.]])\n",
      "Candidate <spatial_scene_grammars_examples.singles_pairs.grammar_constituency.Singles object at 0x7f29be7f04e0> with pose tensor([[1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.]])\n",
      "Candidate <spatial_scene_grammars_examples.singles_pairs.grammar_constituency.Pairs object at 0x7f29be58de80> with pose tensor([[1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.]])\n",
      "Candidate <spatial_scene_grammars_examples.singles_pairs.grammar_constituency.Pairs object at 0x7f29be7f0ac8> with pose tensor([[1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.]])\n",
      "Candidate <spatial_scene_grammars_examples.singles_pairs.grammar_constituency.Pairs object at 0x7f29c7d69a90> with pose tensor([[1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.]])\n",
      "Candidate <spatial_scene_grammars_examples.singles_pairs.grammar_constituency.Pairs object at 0x7f29be58da90> with pose tensor([[1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.]])\n",
      "Candidate <spatial_scene_grammars_examples.singles_pairs.grammar_constituency.Pairs object at 0x7f29c7d69a58> with pose tensor([[1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.]])\n",
      "Candidate <spatial_scene_grammars_examples.singles_pairs.grammar_constituency.Pairs object at 0x7f29be7f2a90> with pose tensor([[1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.]])\n",
      "Candidate <spatial_scene_grammars_examples.singles_pairs.grammar_constituency.Pairs object at 0x7f29be7f20b8> with pose tensor([[1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.]])\n",
      "Candidate <spatial_scene_grammars_examples.singles_pairs.grammar_constituency.Pairs object at 0x7f29be7f24a8> with pose tensor([[1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.]])\n",
      "Candidate <spatial_scene_grammars_examples.singles_pairs.grammar_constituency.Pairs object at 0x7f29be7f2fd0> with pose tensor([[1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.]])\n",
      "Candidate <spatial_scene_grammars_examples.singles_pairs.grammar_constituency.Pairs object at 0x7f29be7f2438> with pose tensor([[1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.]])\n",
      "Candidate <spatial_scene_grammars_examples.singles_pairs.grammar_constituency.Pairs object at 0x7f29c7d696d8> with pose tensor([[1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.]])\n",
      "Candidate <spatial_scene_grammars_examples.singles_pairs.grammar_constituency.Pairs object at 0x7f29be351b38> with pose tensor([[1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.]])\n",
      "Candidate <spatial_scene_grammars_examples.singles_pairs.grammar_constituency.Pairs object at 0x7f29be351630> with pose tensor([[1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.]])\n",
      "Candidate <spatial_scene_grammars_examples.singles_pairs.grammar_constituency.Pairs object at 0x7f29be351668> with pose tensor([[1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.]])\n",
      "5 top-down, 21 bottom-up candidates.\n",
      "Before pruning: 26 candidates\n",
      "After pruning: 5 candidates\n",
      "Singles_71: [[1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]]\n",
      "Pairs_71: [[1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]]\n",
      "Pair_81: [[-0.67205891  0.08026494 -0.73613475  0.36183964]\n",
      " [-0.67497708  0.34243571  0.65356233  1.91989927]\n",
      " [ 0.30453697  0.93610647 -0.17595998 -0.22538756]\n",
      " [ 0.          0.          0.          1.        ]]\n",
      "Pair_82: [[-0.98683119 -0.06198595  0.1494053  -0.34169748]\n",
      " [-0.14607405  0.73822303 -0.65855078  0.30401275]\n",
      " [-0.06947354 -0.67170269 -0.73755605 -0.68901361]\n",
      " [ 0.          0.          0.          1.        ]]\n",
      "Pair_83: [[-0.30106777  0.29289442  0.90750816 -1.12671843]\n",
      " [ 0.15656419 -0.9235666   0.35001771 -0.28575505]\n",
      " [ 0.94066245  0.24746233  0.23219936 -1.09351098]\n",
      " [ 0.          0.          0.          1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Build up a big set of candidate intermediate nodes with both\n",
    "# top-down and bottom-up generation.\n",
    "\n",
    "max_recursion_depth = 10\n",
    "\n",
    "top_down_candidate_intermediate_nodes = generate_top_down_intermediate_nodes_by_supertree(\n",
    "    grammar, observed_nodes, max_recursion_depth=max_recursion_depth\n",
    ")\n",
    "bottom_up_candidate_intermediate_nodes = generate_bottom_up_intermediate_nodes_by_inverting_rules(\n",
    "    grammar, observed_nodes\n",
    ")\n",
    "print(\"%d top-down, %d bottom-up candidates.\" %\n",
    "      (len(top_down_candidate_intermediate_nodes),\n",
    "       len(bottom_up_candidate_intermediate_nodes)))\n",
    "candidate_intermediate_nodes = top_down_candidate_intermediate_nodes + bottom_up_candidate_intermediate_nodes\n",
    "assert all([not node.observed for node in candidate_intermediate_nodes])\n",
    "\n",
    "print(\"Before pruning: %d candidates\" % len(candidate_intermediate_nodes))\n",
    "# Prune duplicates\n",
    "pruned_intermediate_nodes = []\n",
    "for node in candidate_intermediate_nodes:\n",
    "    present = False\n",
    "    for other_node in pruned_intermediate_nodes:\n",
    "        if type(node) is type(other_node) and torch.allclose(node.tf, other_node.tf):\n",
    "            present = True\n",
    "            break\n",
    "    if not present:\n",
    "        pruned_intermediate_nodes.append(node)\n",
    "\n",
    "candidate_intermediate_nodes = pruned_intermediate_nodes            \n",
    "print(\"After pruning: %d candidates\" % len(candidate_intermediate_nodes))\n",
    "for node in candidate_intermediate_nodes:\n",
    "    print(\"%s: %s\" % (node.name, node.tf.detach().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:MIP got 2 solutions, but requested 1. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization success?:  True\n",
      "Logfile: \n",
      "\n",
      "Gurobi 9.0.2 (linux64) logging started Tue Nov 30 03:59:47 2021\n",
      "\n",
      "Gurobi Optimizer version 9.0.2 build v9.0.2rc0 (linux64)\n",
      "Optimize a model with 42 rows, 95 columns and 198 nonzeros\n",
      "Model fingerprint: 0x23c2ae0d\n",
      "Variable types: 0 continuous, 95 integer (95 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 1e+00]\n",
      "  Objective range  [2e-01, 8e+02]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [1e+00, 1e+00]\n",
      "Found heuristic solution: objective 1205.6117474\n",
      "Presolve removed 19 rows and 17 columns\n",
      "Presolve time: 0.00s\n",
      "Presolved: 23 rows, 78 columns, 163 nonzeros\n",
      "Variable types: 0 continuous, 78 integer (78 binary)\n",
      "\n",
      "Root relaxation: objective 6.811424e+02, 30 iterations, 0.00 seconds\n",
      "\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "     0     0  681.14242    0    5 1205.61175  681.14242  43.5%     -    0s\n",
      "H    0     0                     681.3592150  681.14242  0.03%     -    0s\n",
      "\n",
      "Explored 1 nodes (30 simplex iterations) in 0.00 seconds\n",
      "Thread count was 12 (of 12 available processors)\n",
      "\n",
      "Solution count 2: 681.359 1205.61 \n",
      "\n",
      "Optimal solution found (tolerance 1.00e-03)\n",
      "Best objective 6.813592149880e+02, best bound 6.811424249376e+02, gap 0.0318%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set up a MIP to try to glue a valid tree together.\n",
    "# We can build a graph where each node is an observed\n",
    "# or candidate intermediate node, and there is a directed\n",
    "# edge for every legal rule, with a weight corresponding to its\n",
    "# probability. We add one binary variable per edge indicating\n",
    "# its activation.\n",
    "#  - Every observed node except the root needs exactly one active incoming\n",
    "#    edge. The root needs exactly zero. (It shouldn't have any incoming\n",
    "#    edges anyway by assumptions about construction of the grammar.)\n",
    "#    Unobserved nodes have outgoing edges iff they have an active\n",
    "#    incoming edge.\n",
    "#  - The score (and constraints on) a node's set of outgoing edges\n",
    "#    depends on the node type. This includes symmetry breaking\n",
    "#    where appropriate.\n",
    "#  - Maximize the total score.\n",
    "\n",
    "from pydrake.all import (\n",
    "    MathematicalProgram,\n",
    "    GurobiSolver,\n",
    "    SolverOptions\n",
    ")\n",
    "\n",
    "prog = MathematicalProgram()\n",
    "\n",
    "# Extract root node; it may have been observed,\n",
    "# otherwise produce a new one.\n",
    "root = None\n",
    "for node in observed_nodes:\n",
    "    if isinstance(node, grammar.root_node_type):\n",
    "        root = node\n",
    "if root is None:\n",
    "    root = grammar.root_node_type(tf=grammar.root_node_tf)\n",
    "    observed_nodes += [root, ]\n",
    "\n",
    "# For each node, iterate over its rules and add appropriate edges.\n",
    "all_nodes = observed_nodes + candidate_intermediate_nodes\n",
    "# Important to use a MultiDiGraph, as there may be multiple edges\n",
    "# between two nodes (corresponding to different rules being used\n",
    "# to generate the same node).\n",
    "parse_graph = nx.MultiDiGraph()\n",
    "parse_graph.add_nodes_from(all_nodes)\n",
    "def add_edges_for_rule(parent, rule, rule_k, rule_activation_expr):\n",
    "    # Given a parent node and one of its rules, add directed\n",
    "    # edges from this parent node to all children that rule could\n",
    "    # create.\n",
    "    # rule_activation_expr should be a linear expression of\n",
    "    # decision variables that evaluates to 1 when this rule\n",
    "    # is active and 0 when not.\n",
    "    if isinstance(parent, GeometricSetNode):\n",
    "        assert rule_k >= 0 and rule_k <= parent.max_children\n",
    "        assert rule is parent.rules[0]\n",
    "    elif isinstance(parent, (AndNode, OrNode, IndependentSetNode)):\n",
    "        assert rule is parent.rules[rule_k]\n",
    "    else:\n",
    "        raise ValueError(type(rule), \"Bad type.\")\n",
    "    all_outgoing_activations = []\n",
    "    for node in all_nodes:\n",
    "        if isinstance(node, rule.child_type) and node is not parent:\n",
    "            score = rule.score_child(parent, node).detach().item()\n",
    "            var_name = \"%s:%s_%d:%s\" % (parent.name, type(rule).__name__, rule_k, node.name)\n",
    "            if np.isfinite(score):\n",
    "                active = prog.NewBinaryVariables(1, var_name)[0]\n",
    "                parse_graph.add_edge(\n",
    "                    parent, node, active=active, score=score, rule_k=rule_k\n",
    "                )\n",
    "                all_outgoing_activations.append(active)\n",
    "                # If this edge is active, it adds this score to the total cost.\n",
    "                prog.AddLinearCost(-score * active)\n",
    "            else:\n",
    "                logging.warning(\"Skipping rule \", var_name, \" as its infeasible\")\n",
    "    if len(all_outgoing_activations) > 0:\n",
    "        prog.AddLinearConstraint(sum(all_outgoing_activations) == rule_activation_expr)\n",
    "    else:\n",
    "        logging.warning(\"No outgoing connections for %s:%s_%d\" % (parent.name, type(rule), rule_k))\n",
    "    \n",
    "for node in all_nodes:\n",
    "    # Add activation variable for this node.\n",
    "    node.active = prog.NewBinaryVariables(1, node.name + \"_active\")[0]\n",
    "\n",
    "    if isinstance(node, TerminalNode):\n",
    "        # No rules / children to worry about.\n",
    "        continue\n",
    "\n",
    "    elif isinstance(node, AndNode):\n",
    "        # Rules are gated on parent activation.\n",
    "        for rule_k, rule in enumerate(node.rules):\n",
    "            add_edges_for_rule(node, rule, rule_k, node.active)\n",
    "        \n",
    "    elif isinstance(node, OrNode):\n",
    "        activation_vars = prog.NewBinaryVariables(len(node.rules), node.name + \"_outgoing\")\n",
    "        # Rules are gated on parent activation, and exactly one\n",
    "        # is active.\n",
    "        prog.AddLinearConstraint(sum(activation_vars) == node.active)\n",
    "        for rule_k, (rule, activation_var) in enumerate(zip(node.rules, activation_vars)):\n",
    "            add_edges_for_rule(node, rule, rule_k, activation_var)\n",
    "            # Each rule activation has a corresponding score based\n",
    "            # on its log-prob.\n",
    "            prog.AddLinearCost(-activation_var * np.log(node.rule_probs[rule_k].detach().item()))\n",
    "        \n",
    "    elif isinstance(node, IndependentSetNode):\n",
    "        activation_vars = prog.NewBinaryVariables(len(node.rules), node.name + \"_outgoing\")\n",
    "        for rule_k, (rule, activation_var) in enumerate(zip(node.rules, activation_vars)):\n",
    "            add_edges_for_rule(node, rule, rule_k, activation_var)\n",
    "            # The rules are only active if the parent is active.\n",
    "            prog.AddLinearConstraint(activation_var <= node.active)\n",
    "            # Each rule activation incurs an independent score based\n",
    "            # on its log-prob.\n",
    "            prog.AddLinearCost(-activation_var * np.log(node.rule_probs[rule_k].detach().item()))\n",
    "            \n",
    "    elif isinstance(node, GeometricSetNode):\n",
    "        activation_vars = prog.NewBinaryVariables(node.max_children, node.name + \"_outgoing\")\n",
    "        # Ensure that these variables activate in order by constraining\n",
    "        # that for a rule to be active, the preceeding rule must also be\n",
    "        # active.\n",
    "        for k in range(len(activation_vars) - 1):\n",
    "            prog.AddLinearConstraint(activation_vars[k + 1] <= activation_vars[k])\n",
    "        # Ensure at least one is active if the node is active.\n",
    "        # If the node is inactive, then all will be deactivated.\n",
    "        prog.AddLinearConstraint(activation_vars[0] == node.active)\n",
    "        rules = [node.rules[0] for k in range(node.max_children)]\n",
    "        for rule_k, (rule, activation_var) in enumerate(zip(rules, activation_vars)):\n",
    "            add_edges_for_rule(node, rule, rule_k, activation_var)\n",
    "            # Each rule activation incurs an independent score based\n",
    "            # on its log-prob; a rule being active disables the score\n",
    "            # from the previous activation and enables the current score.\n",
    "            if rule_k > 0:\n",
    "                last_score = np.log(node.rule_probs[rule_k - 1].detach().item())\n",
    "            else:\n",
    "                last_score = 0.\n",
    "            this_score = np.log(node.rule_probs[rule_k].detach().item())\n",
    "            prog.AddLinearCost(-activation_var * (-last_score + this_score))\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError(type(node))\n",
    "    \n",
    "# Now that the DiGraph is fully formed, go in an constrain node\n",
    "# activation vars to depend on explanatory incoming edges.\n",
    "for node in observed_nodes:\n",
    "    prog.AddLinearConstraint(node.active == True)\n",
    "for node in all_nodes:\n",
    "    if node is root:\n",
    "        continue\n",
    "    incoming_edges = parse_graph.in_edges(nbunch=node, data=\"active\")\n",
    "    activations = [edge[-1] for edge in incoming_edges]\n",
    "    prog.AddLinearConstraint(sum(activations) == node.active)\n",
    "\n",
    "solver = GurobiSolver()\n",
    "options = SolverOptions()\n",
    "logfile = \"/tmp/gurobi_%s.log\" % datetime.now().strftime(\"%Y%m%dT%H%M%S\")\n",
    "os.system(\"rm -f %s\" % logfile)\n",
    "options.SetOption(solver.id(), \"LogFile\", logfile)\n",
    "options.SetOption(solver.id(), \"MIPGap\", 1E-3)\n",
    "N_solutions = 1\n",
    "if N_solutions > 1:\n",
    "    options.SetOption(solver.id(), \"PoolSolutions\", N_solutions)\n",
    "    options.SetOption(solver.id(), \"PoolSearchMode\", 2)\n",
    "\n",
    "result = solver.Solve(prog, None, options)\n",
    "# Hacky method getter because `num_suboptimal_solution()` was bound with () in its\n",
    "# method name. Should fix this upstream!\n",
    "actual_N_solutions = getattr(result, \"num_suboptimal_solution()\")()\n",
    "if actual_N_solutions != N_solutions:\n",
    "    logging.warning(\"MIP got %d solutions, but requested %d. \", actual_N_solutions, N_solutions)\n",
    "print(\"Optimization success?: \", result.is_success())\n",
    "print(\"Logfile: \")\n",
    "with open(logfile) as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36_pyro",
   "language": "python",
   "name": "py36_pyro"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
