{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Runtime and accuracy scaling experiments\n",
    "\n",
    "For each grammar type (dependency vs constituency), parsing strategy (MIP vs IP), and grammar complexity (by changing the max # number of pairs / singles supported in the grammar), generate a bunch of random environments and collect the top 10 parses. Save out these results for plotting and data analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from collections import namedtuple\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "\n",
    "from spatial_scene_grammars.nodes import *\n",
    "from spatial_scene_grammars.rules import *\n",
    "from spatial_scene_grammars.scene_grammar import *\n",
    "from spatial_scene_grammars.visualization import *\n",
    "\n",
    "# DEPENDENCY\n",
    "DEPENDENCY = 1\n",
    "if DEPENDENCY:\n",
    "    from spatial_scene_grammars_examples.singles_pairs.grammar_dependency import *\n",
    "else:\n",
    "    from spatial_scene_grammars_examples.singles_pairs.grammar_constituency import *\n",
    "    \n",
    "from spatial_scene_grammars.parsing import *\n",
    "from spatial_scene_grammars.sampling import *\n",
    "\n",
    "import meshcat\n",
    "import meshcat.geometry as meshcat_geom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scripts for doing sample runs; warning, long-running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Was tempted to use a class here, but it gives trouble with pickling\n",
    "# unpickling interacting with autoreload. So, ugly dict it is...\n",
    "def make_parsinginfo_dict(\n",
    "    gt_tree, gt_tree_score,\n",
    "    mip_trees, mip_tree_scores,\n",
    "    nlp_trees, nlp_tree_scores,\n",
    "    success, mip_time, nlp_time\n",
    "):\n",
    "    return {\n",
    "        \"gt_tree\": gt_tree,\n",
    "        \"gt_tree_score\": gt_tree_score,\n",
    "        \"mip_trees\": mip_trees,\n",
    "        \"mip_tree_scores\": mip_tree_scores,\n",
    "        \"nlp_trees\": nlp_trees,\n",
    "        \"nlp_tree_scores\": nlp_tree_scores,\n",
    "        \"success\": success,\n",
    "        \"mip_time\": mip_time,\n",
    "        \"nlp_time\": nlp_time\n",
    "    }\n",
    "\n",
    "def parse_random_scenes(N_scenes, N_solutions, strategy):\n",
    "    assert strategy in [\"mip\", \"ip\"]\n",
    "    \n",
    "    grammar = SpatialSceneGrammar(\n",
    "        root_node_type = Root,\n",
    "        root_node_tf = torch.eye(4)\n",
    "    )\n",
    "    parsing_infos = []\n",
    "    for k in tqdm(range(N_scenes), desc=\"Samples\"):\n",
    "        pyro.set_rng_seed(k)\n",
    "        gt_tree = grammar.sample_tree(detach=True)\n",
    "        observed_nodes = gt_tree.get_observed_nodes()\n",
    "        \n",
    "        start = time.time()\n",
    "        if strategy == \"mip\":\n",
    "            inference_results = infer_mle_tree_with_mip(\n",
    "                grammar, observed_nodes, verbose=False, max_scene_extent_in_any_dir=10., N_solutions=10\n",
    "            )\n",
    "            parse_trees = get_optimized_trees_from_mip_results(inference_results)\n",
    "            \n",
    "        elif strategy == \"ip\":\n",
    "            proposed_poses_by_type = generate_candidate_node_pose_sets(\n",
    "                grammar, observed_nodes, max_recursion_depth=10, verbose=False\n",
    "            )\n",
    "            parse_trees = infer_mle_tree_with_mip_from_proposals(\n",
    "                grammar, observed_nodes, proposed_poses_by_type, verbose=False, N_solutions=N_solutions,\n",
    "                min_ll_for_consideration=-1000.\n",
    "            )\n",
    "        mip_done = time.time()\n",
    "        \n",
    "        # NLP refinement\n",
    "        refined_trees = []\n",
    "        for tree in parse_trees:\n",
    "            refinement_result = optimize_scene_tree_with_nlp(grammar, tree, verbose=False)\n",
    "            if not refinement_result.optim_result.is_success():\n",
    "                logging.warning(\"NLP refinement failed.\")\n",
    "            refined_trees.append(refinement_result.refined_tree)\n",
    "        nlp_done = time.time()\n",
    "        \n",
    "        if len(parse_trees) == 0:\n",
    "            logging.error(\"Failed an inference! Results invalid\")\n",
    "            parsing_infos.append(\n",
    "                make_parsinginfo_dict(\n",
    "                    gt_tree=gt_tree,\n",
    "                    gt_tree_score=0.,\n",
    "                    mip_trees=[],\n",
    "                    mip_tree_scores=[],\n",
    "                    nlp_trees=[],\n",
    "                    nlp_tree_scores=[],\n",
    "                    success=False,\n",
    "                    mip_time=0,\n",
    "                    nlp_time=0\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            parsing_infos.append(\n",
    "                make_parsinginfo_dict(\n",
    "                    gt_tree=cleanup_tree_for_pickling(gt_tree),\n",
    "                    gt_tree_score=gt_tree.score(),\n",
    "                    mip_trees=[cleanup_tree_for_pickling(tree) for tree in parse_trees],\n",
    "                    mip_tree_scores=[tree.score() for tree in parse_trees],\n",
    "                    nlp_trees=[cleanup_tree_for_pickling(tree) for tree in refined_trees],\n",
    "                    nlp_tree_scores=[tree.score() for tree in refined_trees],\n",
    "                    success=True,\n",
    "                    mip_time=mip_done-start,\n",
    "                    nlp_time=nlp_done-mip_done\n",
    "                )\n",
    "            )\n",
    "    return parsing_infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run both techniques against this grammar type for a range\n",
    "# of MAX_N_PAIRS and MAX_N_SINGLES, which affect the grammar\n",
    "# \"complexity\" and typical resulting scene size. Key results\n",
    "# by that setting.\n",
    "\n",
    "# Dependency: 1-6 or 7 is reasonable, with patience I might get to 8-10.\n",
    "# Constituency: 1-3 or 1-4. We're in exponential regime.\n",
    "\n",
    "results_by_strategy = {}\n",
    "\n",
    "if DEPENDENCY:\n",
    "    grammar_sizes = range(1, 7)\n",
    "else:\n",
    "    grammar_sizes = range(1, 4)\n",
    "bar = tqdm([\"ip\", \"mip\"])\n",
    "for strategy in bar:\n",
    "    bar.set_description(\"technique: \", strategy)\n",
    "    results_by_param = {}\n",
    "    for k in tqdm(grammar_sizes, desc=\"MAX_N_PAIRS/MAX_N_SINGLES\"):\n",
    "        Pairs.MAX_N_PAIRS = k\n",
    "        Pairs.P = 0.01 # Basically uniform distribution over # of pairs\n",
    "        Singles.MAX_N_SINGLES = k\n",
    "        Singles.P = 0.01 # Basically uniform distribution over # of pairs\n",
    "        results_by_param[k] = parse_random_scenes(N_solutions=10, N_scenes=20, strategy=strategy)\n",
    "    results_by_strategy[strategy] = results_by_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEPENDENCY:\n",
    "    file = \"dependency_scaling_random_runs.pickle\"\n",
    "else:\n",
    "    file = \"constituency_scaling_random_runs.pickle\"\n",
    "\n",
    "OVERWRITE = 1\n",
    "if os.path.exists(file) and not OVERWRITE:\n",
    "    # Merge into existing results\n",
    "    with open(file, \"rb\") as f:\n",
    "        existing_results = pickle.load(f)\n",
    "    for strat_key, results in results_by_strategy.items():\n",
    "        if strat_key not in existing_results.keys():\n",
    "            existing_results[strat_key] = {}\n",
    "        print(strat_key, results.keys())\n",
    "        for key, value in results.items():\n",
    "            if key in existing_results[strat_key].keys():\n",
    "                existing_results[strat_key][key] += value\n",
    "            else:\n",
    "                existing_results[strat_key][key] = value\n",
    "    results_by_strategy = existing_results\n",
    "\n",
    "with open(file, \"wb\") as f:\n",
    "    pickle.dump(results_by_strategy, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results_by_strategy[\"mip\"].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data analysis and plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "with open(\"dependency_scaling_random_runs.pickle\", \"rb\") as f:\n",
    "    dep_results_by_strategy = pickle.load(f)\n",
    "with open(\"constituency_scaling_random_runs.pickle\", \"rb\") as f:\n",
    "    con_results_by_strategy = pickle.load(f)\n",
    "print(dep_results_by_strategy.keys())\n",
    "print(con_results_by_strategy.keys())\n",
    "print(dep_results_by_strategy[\"ip\"].keys())\n",
    "print(con_results_by_strategy[\"ip\"].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot runtime scaling of each technique\n",
    "for k, (strategy, superlabel) in enumerate(zip([\"mip\", \"ip\"], [\"MIP Parser\", \"IP Parser\"])):\n",
    "    plt.figure(dpi=100).set_size_inches(3, 3)\n",
    "    plt.title(superlabel)\n",
    "    con_results = con_results_by_strategy[strategy]\n",
    "    dep_results = dep_results_by_strategy[strategy]\n",
    "\n",
    "    def plot_with_errorbars(xs, list_of_times, label):\n",
    "        y = [np.mean(times) for times in list_of_times]\n",
    "        y_std = [np.std(times) for times in list_of_times]\n",
    "        parts = plt.violinplot(list_of_times, positions=xs, widths=2.)\n",
    "        for pc in parts['bodies']:\n",
    "            c = pc.get_facecolor()\n",
    "            #pc.set_facecolor(c)\n",
    "            #pc.set_edgecolor([0., 0., 0., 0])\n",
    "        for k, (x, times) in enumerate(zip(xs, list_of_times)):\n",
    "            if k == 0:\n",
    "                thislabel = label\n",
    "            else:\n",
    "                thislabel = None\n",
    "            plt.scatter(np.repeat(x, len(times)), times, c=c, label=thislabel)\n",
    "    cm = plt.get_cmap(\"viridis\")\n",
    "\n",
    "    for results, label in zip([con_results, dep_results], [\"Constituency Grammar\", \"Dependency Grammar\"]):\n",
    "        all_times = []\n",
    "        xs = []\n",
    "        for max_entities, info_list in results.items():\n",
    "            all_times.append([\n",
    "                info[\"mip_time\"] for info in info_list\n",
    "            ])\n",
    "            xs.append(max_entities * 3) # Label with # of objects\n",
    "        plot_with_errorbars(xs, all_times, label)\n",
    "\n",
    "    plt.semilogy()\n",
    "    #plt.ylim(0.01, 100.)\n",
    "    plt.grid(True)\n",
    "    plt.ylabel(\"Parsing runtime (s)\")\n",
    "    plt.xlabel(\"Grammar size\\n(Maximum # of objects)\")\n",
    "    plt.legend(bbox_to_anchor=(1., 1.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For all examples we collected for each parse and grammar type,\n",
    "# collect the histogram of the best parse score vs the ground truth\n",
    "# tree score.\n",
    "for i, (strategy, superlabel) in enumerate(zip([\"mip\", \"ip\"], [\"MIP Parser\", \"IP Parser\"])):\n",
    "    plt.figure(dpi=300).set_size_inches(6, 6)\n",
    "    plt.suptitle(superlabel)\n",
    "    con_results = con_results_by_strategy[strategy]\n",
    "    dep_results = dep_results_by_strategy[strategy]\n",
    "\n",
    "    for j, (results, label) in enumerate(zip([con_results, dep_results], [\"Constituency Grammar\", \"Dependency Grammar\"])):\n",
    "        plt.subplot(2, 2, 2*j+1)\n",
    "        relative_mip_scores = []\n",
    "        relative_nlp_scores = []\n",
    "        for info_list in results.values():\n",
    "            for info in info_list:\n",
    "                relative_mip_scores.append(max(info[\"mip_tree_scores\"]).item() - info[\"gt_tree_score\"].item())\n",
    "                relative_nlp_scores.append(max(info[\"nlp_tree_scores\"]).item() - info[\"gt_tree_score\"].item())\n",
    "        if not all(np.isfinite(relative_mip_scores)):\n",
    "            logging.error(\"Had infinite mip score?\")\n",
    "        plt.hist(relative_mip_scores, density=True, alpha=0.5, label=\"MIP\")\n",
    "        mip_success_ratio = sum(np.array(relative_mip_scores) >= -1E-3) / len(relative_mip_scores)\n",
    "        plt.title(label)\n",
    "        \n",
    "        plt.subplot(2, 2, 2*j+2)\n",
    "        if not all(np.isfinite(relative_nlp_scores)):\n",
    "            logging.error(\"Had infinite nlp score?\")\n",
    "            relative_nlp_scores = np.array(relative_nlp_scores)[np.isfinite(relative_nlp_scores)]\n",
    "        plt.hist(relative_nlp_scores, density=True, alpha=0.5, label=\"NLP\")\n",
    "        nlp_success_ratio = sum(np.array(relative_nlp_scores) >= -1E-3) / len(relative_nlp_scores)\n",
    "        plt.grid(True)\n",
    "        plt.xlabel(\"Best parse score - GT parse score\")\n",
    "        \n",
    "        print(\"%s:%s MIP %f success, NLP %f success at matching or beating GT tree\" % (superlabel, label, mip_success_ratio, nlp_success_ratio))\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize failed parses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_trees = []\n",
    "for key, infos in con_results_by_strategy[\"mip\"].items():\n",
    "    for info in infos:\n",
    "        if max(info[\"nlp_tree_scores\"]) < -150:\n",
    "            failed_trees.append((key, info))\n",
    "            \n",
    "print(\"Failed trees: \", failed_trees)\n",
    "print(\"# of failed sets: \", len(failed_trees))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up mushcat\n",
    "if 'vis' not in globals():\n",
    "    vis = meshcat.Visualizer()\n",
    "vis.delete()\n",
    "base_url = \"http://127.0.0.1\"\n",
    "meshcat_url = base_url + \":\" + vis.url().split(\":\")[-1]\n",
    "print(\"Meshcat url: \", meshcat_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar_param, failed_info = failed_trees[0]\n",
    "\n",
    "gt_tree = failed_info[\"gt_tree\"]\n",
    "failed_tree = failed_info[\"nlp_trees\"][0]\n",
    "bad_nlp_tree_score = failed_tree.score(verbose=True)\n",
    "print(bad_nlp_tree_score)\n",
    "\n",
    "Pairs.MAX_N_PAIRS = grammar_param\n",
    "Pairs.P = 0.01 # Basically uniform distribution over # of pairs\n",
    "Singles.MAX_N_SINGLES = grammar_param\n",
    "Singles.P = 0.01 # Basically uniform distribution over # of pairs\n",
    "\n",
    "draw_scene_tree_contents_meshcat(gt_tree, zmq_url=vis.window.zmq_url, prefix=\"gt_tree\")\n",
    "draw_scene_tree_structure_meshcat(gt_tree, zmq_url=vis.window.zmq_url, prefix=\"gt_tree_structure\")\n",
    "draw_scene_tree_structure_meshcat(failed_tree, zmq_url=vis.window.zmq_url, prefix=\"failed_tree_structure\")\n",
    "print(\"Score: \", failed_tree.score())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
