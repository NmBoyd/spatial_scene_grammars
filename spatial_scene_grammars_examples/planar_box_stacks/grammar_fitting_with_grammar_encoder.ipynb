{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fb7162ecd38>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2    \n",
    "# Autoreload isn't working for the ssg stuff,\n",
    "# unclear why, I think it has too many layers of imports.\n",
    "\n",
    "from collections import namedtuple\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from functools import reduce \n",
    "import operator\n",
    "def prod(iterable):\n",
    "    return reduce(operator.mul, iterable, 1)\n",
    "\n",
    "import torch\n",
    "import torch.distributions.constraints as constraints\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.contrib.autoname import scope\n",
    "\n",
    "from spatial_scene_grammars.nodes import *\n",
    "from spatial_scene_grammars.rules import *\n",
    "from spatial_scene_grammars.tree import *\n",
    "from spatial_scene_grammars.sampling import *\n",
    "from spatial_scene_grammars.torch_utils import *\n",
    "from spatial_scene_grammars.neural_grammar_proposal import *\n",
    "\n",
    "from spatial_scene_grammars_examples.planar_box_stacks.grammar import *\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a bigger dataset of observed trees, and prepare meta-tree.\n",
    "full_trees_for_training = []\n",
    "observed_nodes_for_training = []\n",
    "def make_dataset(N=100):\n",
    "    full_trees = []\n",
    "    observed_nodes = []\n",
    "    for k in range(N):\n",
    "        scene_trees, success = sample_tree_from_root_type_with_constraints(\n",
    "            root_node_type=Ground,\n",
    "            root_node_instantiation_dict={\n",
    "                \"xy\": torch.tensor([0., 0.])\n",
    "            },\n",
    "            constraints=[\n",
    "                NonpenetrationConstraint(0.001),\n",
    "            ],\n",
    "            max_num_attempts=1000,\n",
    "            backend=\"rejection\",#\"metropolis_procedural_modeling\",\n",
    "        )\n",
    "        if not success:\n",
    "            continue\n",
    "        full_trees.append(scene_trees[0])\n",
    "        observed_nodes.append([n for n in scene_trees[0].nodes if isinstance(n, TerminalNode)])\n",
    "    return full_trees, observed_nodes\n",
    "full_trees_for_training, observed_nodes_for_training = make_dataset(500)\n",
    "full_trees_for_testing, observed_nodes_for_testing = make_dataset(250)\n",
    "meta_tree = SceneTree.make_meta_scene_tree(Ground())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load encoder from saved run.\n",
    "encoder = GrammarEncoder(meta_tree, 512)\n",
    "weight_path = \"saved_models/20210319/encoder_supervised_2.5792.torch\"\n",
    "#weight_path = \"saved_models/20210319/encoder_unsupervised_-6.0246.torch\"\n",
    "encoder.load_state_dict(torch.load(weight_path))\n",
    "encoder.eval()\n",
    "x = encoder(observed_nodes_for_training[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grammar tuning given a neural proposal engine:\n",
    "\n",
    "The most direct and appealing approach would be: can I just encode everything in my dataset and convert the meta-tree encoding to grammar encodings? The core question is how to back out the equivalent restricted-grammar params for a set of meta-tree params. Continuous values are already done (the mean-field approx is organized per-node-type). Discrete values require combination across the places the node in question appears in the meat-scene tree: I *think* simple weighted-averaging across the node's appearances will work. Weighting is by how often that node is going to appear in the final grammar, which we can calculate by looking at the parent production probs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Err:  tensor(0.0166, grad_fn=<SumBackward0>)\n",
      "Err:  tensor(0.0436, grad_fn=<SumBackward0>)\n",
      "Err:  tensor(0.1326, grad_fn=<SumBackward0>)\n",
      "Err:  tensor(0.0172, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Utility we'll need for collating tree info.\n",
    "\n",
    "from networkx.drawing.nx_agraph import graphviz_layout\n",
    "plt.figure(dpi=300).set_size_inches(24, 8)\n",
    "N = 10\n",
    "for k in range(N):\n",
    "    plt.subplot(3, N, k + 1)\n",
    "    draw_boxes(full_trees_for_training[k], fig=plt.gcf(), ax=plt.gca(), block=False)\n",
    "    def draw(x):\n",
    "        inclusion_log_likelihood_per_node, _ = encoder.get_product_weights_and_inclusion_lls(meta_tree, x)\n",
    "        nx.draw_networkx(\n",
    "            meta_tree,\n",
    "            ax=plt.gca(),\n",
    "            pos = graphviz_layout(meta_tree),\n",
    "            labels={node: node.__class__.__name__ for node in meta_tree},\n",
    "            node_color=[inclusion_log_likelihood_per_node[node] for node in meta_tree]\n",
    "        \n",
    "    )\n",
    "    plt.subplot(3, N, N + k + 1)\n",
    "    x_target = encoder.get_grammar_parameters_from_actual_tree(meta_tree, full_trees_for_training[k])\n",
    "    draw(x_target)\n",
    "    plt.subplot(3, N, 2*N + k + 1)\n",
    "    x = encoder(observed_nodes_for_training[k])\n",
    "    draw(x)\n",
    "    \n",
    "    # Penalize distance for non-nan terms\n",
    "    target_mask = torch.isfinite(x_target)\n",
    "    err = (x - x_target)[target_mask].square().sum()\n",
    "    print(\"Err: \", err)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode every observation in the training set, and record the resulting\n",
    "# implied distribution over continuous and discrete grammar parameters\n",
    "# for each node type.\n",
    "\n",
    "# Each of these are keyed by node type, with lists of distributions\n",
    "# as value.\n",
    "local_variable_dists_by_node_type = {}\n",
    "derived_variable_dists_by_node_type = {}\n",
    "# This one is a list of (weight_scalar, child weight vectors).\n",
    "product_weights_with_weight_by_node_type = {}\n",
    "\n",
    "for node in meta_tree:\n",
    "    node_type = type(node)\n",
    "    if node_type not in local_variable_dists_by_node_type.keys():\n",
    "        local_variable_dists_by_node_type[node_type] = list()\n",
    "        derived_variable_dists_by_node_type[node_type] = list()\n",
    "        product_weights_with_weight_by_node_type[node_type] = list()\n",
    "all_node_types = list(local_variable_dists_by_node_type.keys())\n",
    "\n",
    "for observed_nodes in observed_nodes_for_training:\n",
    "    x = encoder(observed_nodes_for_training[0])\n",
    "    \n",
    "    inclusion_log_likelihood_per_node, product_weights_per_node = \\\n",
    "        get_product_weights_and_inclusion_lls(meta_tree, x)\n",
    "    print(\"Inclusion ll per node: \", inclusion_log_likelihood_per_node)\n",
    "    nx.draw_networkx(\n",
    "        meta_tree,\n",
    "        pos = graphviz_layout(meta_tree),\n",
    "        labels={node: node.__class__.__name__ for node in meta_tree}\n",
    "    )\n",
    "    \n",
    "    \n",
    "    break\n",
    "\n",
    "    for meta_node in list(meta_tree.nodes):\n",
    "        # Extract product weight dists:\n",
    "        if isinstance(meta_node, NonTerminalNode) and not isinstance(meta_node, AndNode):\n",
    "            assert isinstance(meta_node, NonTerminalNode)\n",
    "            inds = encoder.node_output_info[meta_node].product_weight_inds\n",
    "            raw_product_weights = x[inds]\n",
    "            if isinstance(meta_node, OrNode):\n",
    "                product_weights = torch.nn.functional.softmax(raw_product_weights, dim=0)\n",
    "            elif isinstance(meta_node, IndependentSetNode):\n",
    "                product_weights = torch.sigmoid(raw_product_weights)\n",
    "            elif isinstance(meta_node, GeometricSetNode):\n",
    "                # Choose count\n",
    "                product_weights = torch.nn.functional.softmax(raw_product_weights, dim=0)\n",
    "            else:\n",
    "                raise NotImplementedError(\"Don't know how to decode Nonterminal type %s\" % meta_node.__class__.__name__)\n",
    "            product_weights_per_node[meta_node] = product_weights\n",
    "\n",
    "            \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
